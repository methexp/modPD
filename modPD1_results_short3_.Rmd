---
editor_options:
  chunk_output_type: console
output:
  pdf_document: default
  html_document: default
---

```{r prepare_variables, include=FALSE, echo = FALSE, warning=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, cache=TRUE)
options(scipen=1, digits=2)

library("papaja")
library(kableExtra)
library(stringr)
library(afex)
library(bfrr)
library(dplyr)
library(effectsize)
set_sum_contrasts()

#function to display results from Bayesian analyses conducted with the package bfrr
printBFRR <- function(bf, RR=TRUE){
  result = paste0(" $BF_{H1: ", bf$H1_model, "}=$ ", ifelse((bf$BF>999)|(bf$BF<.001), format(bf$BF, scientific=TRUE), format(bf$BF, digits=2)))
  if(RR) result <- paste0(result, " (Robustness Region: ", bf$RR$sd[1], ", ", bf$RR$sd[2], ")" ) 
  #print(result) # use for debugging in the console
  result
}

# load data

load(file="./data/modPD1.RData")
d1 <- subset(d1, Condition !="Exclusion")
n1 <- length(unique(d1$Subject))

# prepare variables

d1$Valence <- d1$typUS
d1$`Pairing`[d1$typCS=="neutral"] <- "paired" 
d1$`Pairing`[d1$typCS=="neg" | d1$typCS=="pos"] <- "nonpaired"
d1p <- subset(d1, Pairing=="paired")
d1np <- subset(d1, Pairing=="nonpaired")
d1$`Evaluative change` <- d1$postrating - d1$prerating
d1$`TBS Set` <- d1$pdset

d1$pdcor <- FALSE
d1$pdcor[d1$pdval == "pleasant" & d1$typUS == "pos"] <- TRUE
d1$pdcor[d1$pdval == "unpleasant" & d1$typUS == "neg"] <- TRUE
d1$pdcor_ <- as.numeric(d1$pdcor)

```


## Results

We first report results of a manipulation check (i.e., whether our study produced EC effects).
The responses on the novel TBS task are analyzed next, first assessing its validity (i.e., whether the memory and attitude manipulations were adequately reflected in the responses), and then testing for EC without memory as well as the attitude-as-information bias.
<!-- Third, we related the responses on the novel task to evaluative ratings. -->

<!-- ### Manipulation checks -->

<!-- Evaluations and memory judgments as a function of experimental factors -->
<!-- Evaluations by Time (pre-post), Pairing, and Valence -->
<!-- Memory by Pairing and Valence -->

### Evaluative conditioning

Evaluative change was computed as post-learning evaluation minus pre-learning evaluation and submitted to a 2 (Valence) x 2 (Pairing) repeated-measures ANOVA.
Figure \@ref(fig:ec1appendix) shows the results.

```{r ec1appendix, fig.cap="EC effects (pre-post evaluative changes) as a function of Valence and Pairing"}

#par(mfrow=c(1,2))
par(cex = 1.0)

ec1 <- aov_ez(data=d1, id="Subject", dv="Evaluative change", within=c("Valence","Pairing"), fun_aggregate = mean)
ec1_out <- apa_print(ec1)

apa_beeplot(data=d1, id="Subject", dv="Evaluative change", factors=c("Pairing","Valence"), intercept=0, ylim=c(-25,25))

## EC for M vs. A
d1MA <- subset(d1, typCS=="neutral" & Pairing=="paired")
ec_MA <- aov_ez(data=d1MA, id="Subject", dv="Evaluative change", within=c("Valence","pdset"), fun_aggregate = mean)
#apa_beeplot(data=d1MA, id="Subject", dv="Evaluative change", factors=c("pdset","Valence"), intercept=0, ylim=c(-25,25))

d1M <- subset(d1, typCS=="neutral" & pdset=="Memory" & Pairing=="paired")
d1A <- subset(d1, typCS=="neutral" & pdset=="Attitude" & Pairing=="paired")
ec_1M <- aov_ez(data=d1M, id="Subject", dv="Evaluative change", within=c("Valence"), fun_aggregate = mean)
ec_1A <- aov_ez(data=d1A, id="Subject", dv="Evaluative change", within=c("Valence"), fun_aggregate = mean)
#apa_beeplot(data=d1A, id="Subject", dv="Evaluative change", factors=c("Valence"), intercept=0, ylim=c(-25,25))

```

EC was affected by Valence, `r ec1_out$full_result$Valence`, Pairing, `r ec1_out$full_result$Pairing`, as well as, most strongly, their interaction, `r ec1_out$full_result$Valence_Pairing`.
For nonpaired CSs, pre-post changes reflected the statistical artifact of regression to the mean: 
The CSs with most extremely positive pre-ratings became less positive, and the CSs with most extremely negative pre-ratings became less negative.
For paired CSs, pre-post evaluative changes reflected US valence (i.e., an EC effect): 
Those paired with positive CSs became more positive, while those paired with negative USs became more negative.
Interestingly, there was an asymmetry, with the positive larger than the negative shift.

### Validation of the Two-button-sets procedure

To show that the modified task adequately assesses metamemory judgments of CS-US pairings, and attitudes for cases in which participants do not remember CS-US pairings, we analyzed whether (1) pairing affects participants' Memory-set reports (which we expected to differ between paired versus nonpaired control CSs); (2) those memory reports are largely accurate; and (3) participants' pre-rated attitudes are consistently reflected in the Attitude button responses.


```{r freqtable}

## frequencies

### Set selection: Mem vs. Att

p <- with(subset(d1, Pairing=="paired"), table(pdset)) 
np <- with(subset(d1, Pairing=="nonpaired"), table(pdset)) 
p_ <- c(rep(paste0(p[1], " (", printp(p[1]/sum(p[1:2]),2),")"),2)
        , rep(paste0(p[2], " (", printp(p[2]/sum(p[1:2]),2),")"),2))
np_ <- c(rep(paste0(np[1], " (", printp(np[1]/sum(np[1:2]),2),")"),2)
        , rep(paste0(np[2], " (", printp(np[2]/sum(np[1:2]),2),")"),2))

p_ <- c(p[1], paste0(" (", printp(p[1]/sum(p[1:2]),2),")")
        , p[2], paste0(" (", printp(p[2]/sum(p[1:2]),2),")"))
np_ <- c(np[1], paste0(" (", printp(np[1]/sum(np[1:2]),2),")")
        , np[2], paste0(" (", printp(np[2]/sum(np[1:2]),2),")"))

## pleas/unpleas

ppos <- with(subset(d1, Pairing=="paired" & Valence=="pos"), table(pdset, pdval)) # pdresp: 1,3=pleas, 2,4=unpl
ppos_ <- c(ppos[1,1], ppos[1,2], ppos[2,1], ppos[2,2])

pneg <- with(subset(d1, Pairing=="paired" & Valence=="neg"), table(pdset, pdval)) # pdresp: 1,3=pleas, 2,4=unpl
pneg_ <- c(pneg[1,1], pneg[1,2], pneg[2,1], pneg[2,2])

nppos <- with(subset(d1, Pairing=="nonpaired" & Valence=="pos"), table(pdset, pdval)) # pdresp: 1,3=pleas, 2,4=unpl
nppos_ <- c(nppos[1,1], nppos[1,2], nppos[2,1], nppos[2,2])

npneg <- with(subset(d1, Pairing=="nonpaired" & Valence=="neg"), table(pdset, pdval)) # pdresp: 1,3=pleas, 2,4=unpl
npneg_ <- c(npneg[1,1], npneg[1,2], npneg[2,1], npneg[2,2])

ppos__ <- c( paste0(ppos_[1], " (", printp(ppos_[1]/sum(ppos_[1:2], pneg_[1:2]),digits = 2),")")
            ,paste0(ppos_[2], " (", printp(ppos_[2]/sum(ppos_[1:2], pneg_[1:2]),digits = 2),")")
            ,paste0(ppos_[3], " (", printp(ppos_[3]/sum(ppos_[3:4], pneg_[3:4]),digits = 2),")")
            ,paste0(ppos_[4], " (", printp(ppos_[4]/sum(ppos_[3:4], pneg_[3:4]),digits = 2),")"))

pneg__ <- c( paste0(pneg_[1], " (", printp(pneg_[1]/sum(ppos_[1:2], pneg_[1:2]),digits = 2),")")
            ,paste0(pneg_[2], " (", printp(pneg_[2]/sum(ppos_[1:2], pneg_[1:2]),digits = 2),")")
            ,paste0(pneg_[3], " (", printp(pneg_[3]/sum(ppos_[3:4], pneg_[3:4]),digits = 2),")")
            ,paste0(pneg_[4], " (", printp(pneg_[4]/sum(ppos_[3:4], pneg_[3:4]),digits = 2),")"))

nppos__ <- c(paste0(nppos_[1], " (", printp(nppos_[1]/sum(nppos_[1:2], npneg_[1:2]),digits = 2),")")
            ,paste0(nppos_[2], " (", printp(nppos_[2]/sum(nppos_[1:2], npneg_[1:2]),digits = 2),")")
            ,paste0(nppos_[3], " (", printp(nppos_[3]/sum(nppos_[3:4], npneg_[3:4]),digits = 2),")")
            ,paste0(nppos_[4], " (", printp(nppos_[4]/sum(nppos_[3:4], npneg_[3:4]),digits = 2),")"))

npneg__ <- c(paste0(npneg_[1], " (", printp(npneg_[1]/sum(nppos_[1:2], npneg_[1:2]),digits = 2),")")
            ,paste0(npneg_[2], " (", printp(npneg_[2]/sum(nppos_[1:2], npneg_[1:2]),digits = 2),")")
            ,paste0(npneg_[3], " (", printp(npneg_[3]/sum(nppos_[3:4], npneg_[3:4]),digits = 2),")")
            ,paste0(npneg_[4], " (", printp(npneg_[4]/sum(nppos_[3:4], npneg_[3:4]),digits = 2),")"))

## chisq indep tests

memsubj_test <- chisq.test(matrix(c(p,np), nrow=2))
memacc_test <- chisq.test(matrix(c(ppos[1,1:2], pneg[1,1:2]), nrow=2))
attval_test <- chisq.test(matrix(c(nppos[2,], npneg[2,]), nrow=2))
ec_wo_mem_test <- chisq.test(matrix(c(ppos[2,], pneg[2,]), nrow=2))
consbias_test <- chisq.test(matrix(c(nppos[1,1:2], npneg[1,1:2]), nrow=2))

freqtable <- data.frame("M" = c(rep("M",2), rep("A",2))
                       , "CS"=p_ #c(rep(.64,2), rep(.36,2))
                       , "Foil"=np_ #c(rep(.08,2), rep(.92,2))
                       , "Val"=rep(c("+","-"),2)
                       , "CS.p"=ppos__ #c(ppos[1,1], ppos[1,2], ppos[2,3], ppos[2,4])
                       , "CS.n"=pneg__ #c(pneg[1,1], pneg[1,2], pneg[2,3], pneg[2,4])
                       , "Foil.p"=nppos__ #c(nppos[1,1], nppos[1,2], nppos[2,3], nppos[2,4])
                       , "Foil.n"= npneg__ #c(npneg[1,1], npneg[1,2], npneg[2,3], npneg[2,4])
                  )

kbl(freqtable, booktabs=TRUE, align="c", caption="Response frequencies (proportions) on the TBS task, separated by pairing status and US valence (Exp.1). ") %>%
  kable_styling(latex_options = c("scale_down")) %>%
  collapse_rows(columns = 1, latex_hline = "major", valign = "middle")
# Chisq tests are reported for M responses to paired stimuli (memory accuracy), A responses to paired stimuli (EC without memory), M responses to nonpaired stimuli (attitude-consistent guessing), and A responses to nonpaired stimuli (pre-study attitudes).

print_chisq <- function(d){
  paste0("Chisq(",d$parameter,") = ", round(d$statistic,2), ", p = ", printp(d$p.value))
}

```


```{r exp1_logist}


# M/A judgments

## all data
x1. <- glmer(pdset ~ Valence*Pairing + (1 + Valence * Pairing|Subject), family = binomial, data=subset(d1), control=glmerControl(optimizer = "bobyqa"))

## paired
x1.p <- glmer(pdset ~ Valence + (1 + Valence*Pairing|Subject), family = binomial, data=subset(d1), control=glmerControl(optimizer = "bobyqa"))

## nonpaired
x1.u <- glmer(pdset ~ Valence + (1 + Valence|Subject), family = binomial, data=subset(d1np), control=glmerControl(optimizer = "bobyqa"))


# un/pleasant judgments

## all data
x1 <- glmer(pdval ~ Valence*Pairing*pdset + (1 + Valence*Pairing|Subject), family = binomial, data=subset(d1), control=glmerControl(optimizer = "bobyqa"))

## paired
x1p <- glmer(pdval ~ Valence*pdset + (1 + Valence|Subject), family = binomial, data=subset(d1p), control=glmerControl(optimizer = "bobyqa"))

## paired, Memory
x1pM <- glmer(pdval ~ Valence + (1 + Valence|Subject), family = binomial, data=subset(d1p, pdset=="Memory"), control=glmerControl(optimizer = "bobyqa"))

## paired, Attitude
x1pA <- glmer(pdval ~ Valence + (1 + Valence|Subject), family = binomial, data=subset(d1p, pdset=="Attitude"), control=glmerControl(optimizer = "bobyqa"))

## nonpaired
x1np <- glmer(pdval ~ Valence*pdset + (1 + Valence|Subject), family = binomial, data=subset(d1np), control=glmerControl(optimizer = "bobyqa"))

## nonpaired, Memory
x1npM <- glmer(pdval ~ Valence + (1 + Valence|Subject), family = binomial, data=subset(d1np, pdset=="Memory"), control=glmerControl(optimizer = "bobyqa"))

## nonpaired, Attitude
x1npA <- glmer(pdval ~ Valence + (1 + Valence|Subject), family = binomial, data=subset(d1np, pdset=="Attitude"), control=glmerControl(optimizer = "bobyqa"))

```

```{r tbs_correct_proportions, include=FALSE}
#keep only neutral paired CSs and participants that performed the TBS
dat_tbs = subset(d1, Pairing == "paired" & typCS == "neutral") %>% droplevels()
dat_tbs$pdset = as.factor(dat_tbs$pdset)

#for each participant, compute the proportion of correct identifications in the TBS
#meaning, say "positive" if the CS was paired with a positive US, and say "negative if the CS was paired with a negative US
dat_tbs$pdval_pleasant = ifelse(dat_tbs$pdval == "pleasant", "pos", "neg")

dat_tbs$tbs_correct = as.factor(ifelse(dat_tbs$pdval_pleasant == dat_tbs$typUS, "correct", "incorrect"))

prop_correct_tbs = dat_tbs %>% 
  group_by(Subject, tbs_correct, .drop=FALSE) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(Subject) %>% filter(!is.nan(freq))

colnames(prop_correct_tbs) = c("subject", "correct_response", "n_count", "prop")

#proportion correct only for memory buttons in the TBS
prop_correct_tbs_mem = dat_tbs %>% filter(pdset == "Memory") %>%
  group_by(Subject, tbs_correct, .drop=FALSE) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(Subject) %>% filter(!is.nan(freq))

colnames(prop_correct_tbs_mem) = c("subject", "correct_response", "n_count", "prop")

#tests
t_prop_correct_m = t.test(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"], mu = 0.5)
d_prop_correct_m = cohens_d(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"], mu = .5)

bf_prop_tbs_memory = bfrr(
  sample_mean = mean(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"])-.5, # mean of the sample
  sample_se = sd(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"])/sqrt(length(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"])), # SE of the sample
  sample_df = length(prop_correct_tbs_mem$prop[prop_correct_tbs_mem$correct_response=="correct"]) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = .15, 
  tail = 1, #one-tailed
  criterion = 3, 
  rr_interval = list( # ranges to vary H1 parameters for robustness regions
    sd = c(0, 2) 
  )
)
#summary(bf_prop_tbs_memory)

#proportion correct only for attitude buttons in the TBS
prop_correct_tbs_att = dat_tbs %>% filter(pdset == "Attitude") %>%
  group_by(Subject, tbs_correct, .drop=FALSE) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>% group_by(Subject) %>% filter(!is.nan(freq))

colnames(prop_correct_tbs_att) = c("subject", "correct_response", "n_count", "prop")

#tests
t_prop_correct_a = t.test(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"], mu = 0.5)
d_prop_correct_a = cohens_d(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"], mu = .5)

bf_prop_tbs_att = bfrr(
  sample_mean = mean(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])-.5, # mean of the sample
  sample_se = sd(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])/sqrt(length(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])), # SE of the sample
  sample_df = length(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"]) - 1, # degrees of freedom
  model = "normal",
  mean = 0, 
  sd = .15, 
  tail = 1, #one-tailed
  criterion = 3, 
  rr_interval = list( # ranges to vary H1 parameters for robustness regions
    sd = c(0, 2) 
  )
)
#summary(bf_prop_tbs_att)
#plot(bf_prop_tbs_att) 
# --> Evidence for H0s with larger effects (i.e., SD > 25%)
```

Table \@ref(tab:freqtable) shows response frequencies on the TBS task.
The first columns show that participants' metamemory judgments clearly distinguished quite well between paired and nonpaired stimuli, `r print_chisq(memsubj_test)`.
^[A logistic regression confirmed that participants reported more Memory responses for the former than the latter (and vice versa), `r apa_print(x1.)$full_result$Pairing1`. A model with only Pairing predictor performed better than models including the Valence factor and/or the interaction.]

Next, we analyzed the frequencies of "pleasant"/"unpleasant" responses.
It varied as a function of Valence, `r apa_print(x1)$full_result$Valence1`, and this effect was modulated by a two-way interaction with Pairing, `r apa_print(x1)$full_result$Valence1_Pairing1`, as well as its three-way interaction with Response Set (Memory vs. Attitude), `r apa_print(x1)$full_result$Valence1_Pairing1_pdset1`.
Separate analyses for paired and nonpaired stimuli showed, for both item types, effects of Valence (paired: `r apa_print(x1p)$full_result$Valence1`; nonpaired: `r apa_print(x1np)$full_result$Valence1`).
Importantly, they also showed interactions of Valence with Response-Set (paired: `r apa_print(x1p)$full_result$Valence1_pdset1`; nonpaired: `r apa_print(x1np)$full_result$Valence1_pdset1`), indicating that Valence differentially affects Memory and Attitude responses. 
We therefore investigated the effect of Valence separately for each of the four subsets of the Pairing (paired CS vs. nonpaired foil) x Response Set (Memory vs. Attitude).

The corresponding aggregate counts are given in the four 2-by-2 quadrants of Table 1 (one for each combination of the Pairing and Mem/Att variables).
The top left quadrant (i.e., Memory judgments for paired stimuli) is relevant for evaluating accuracy of Memory responses.
It shows frequencies (and proportions, in parentheses) of 'pleasant' (Val:+) and 'unpleasant' (Val:-) responses for CSs, which were paired either with positive (CS.p) or negative (CS.n) USs.
Memory judgments for paired stimuli were affected by (and quite accurately reflected) actual US valence, `r apa_print(x1pM)$full_result$Valence1`.
When participants experienced remembering US valence for a paired stimulus, they were accurate in 80% of cases.

The bottom right quadrant (i.e., Attitude judgments for nonpaired stimuli) informs us about the task's ability to reflect pre-experimental attitude.
Nonpaired stimuli (i.e., those not presented together with USs) were selected to be either clearly positive or negative for a given participant.
Attitude judgments for nonpaired valent stimuli were strongly influenced by (and quite accurately reflected) participants' pre-study attitudes towards these stimuli, `r  apa_print(x1npA)$full_result$Valence1`.
Participants consistently reported the attitude corresponding to their pre-study ratings in 95% of cases.

So far, the results show that the TBS task adequately reflected experimental manipulations:
Participants' metamemory judgments were able to separate paired from nonpaired stimuli; Memory-set judgments for paired CSs were largely accurate; and Attitude-set judgments reflected participants' attitudes quite well.

### Is evaluative conditioning observed in the absence of conscious retrieval of the CS-US pairings?

The remaining two quadrants inform us about two phenomena of the relation between EC and memory:
EC in the absence of retrieval awareness (bottom-left quadrant), and attitude-as-information bias (top-right quadrant).

The bottom left quadrant (Attitude-set judgments for paired stimuli) is relevant for the issue of EC in the absence of memory:
An Attitude-set judgment implies the metacognitive absence of remembering US valence. 
For paired CSs, the Attitude-set responses should reflect evaluations in the absence of US valence memory.
We saw above that Valence affected responses to paired CSs, and that this effect was modulated by an interaction with Response-Button set:
Whereas the effect of Valence was found for Memory-set responses (see above), it was absent from Attitude-set responses, `r apa_print(x1pA)$full_result$Valence1`. 
Thus, as reflected in Table 2, there was no significant EC in the absence of memory.

Because failing to find a significant effect is not evidence for the absence of an effect, we additionally conducted a Bayesian analysis that allows quantifying the evidence for the null hypothesis of no effect. 
Specifically, for Attitude-set judgments on paired neutral CSs, we computed a Bayes Factor (BF) on the proportions of responses in line with US Valence. 
We modeled the distribution of true effect sizes under the H1 as a half-normal distribution with an SD of 15% (similar to Waroquier et al., 2020; they chose this value based on Stahl et al., 2009, who found valence memory performance roughly 15% above the 50% chance level; we assume here that effects on memory and evaluations should be comparable under a single-process account). 
For each BF (noted $BF_{H1:N(0,SD)}$, e.g., noted $BF_{H1:N(0,.15)}$), we report the Robustness Region to indicate the range of *SD*s of the H1 distribution that support the same conclusion as the reported BF. 
We used conventional cut-offs to interpret BF (e.g., Dienes, 2014). 
A BF above 3 yields evidence for H1 compared with H0, while a BF below 1/3 yields evidence for H0. 
A BF between 1/3 and 3 would count as anecdotal evidence in either direction, and indicates inconclusive data (i.e., no evidence for either hypothesis).

The Bayesian analysis yielded inconclusive evidence for or against above-chance responses in line with US valence (*M =* `r mean(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])`; *SD =* `r sd(prop_correct_tbs_att$prop[prop_correct_tbs_att$correct_response=="correct"])`), `r printBFRR(bf_prop_tbs_att)`.
^[The robustness region shows, however, that the data provide evidence against H1 distributions with SDs greater than .25.]
In sum, we failed to find an EC effect for Attitude-set judgments, but we cannot conclude from this finding that there is evidence for no EC in the absence of memory.

### Is there evidence for an affect-as-information bias?

Finally, we probed whether Memory-set responses were systematically affected by participants' attitudes towards nonpaired (but inherently valent) stimuli.
The top-right quadrant of Table 1 (Memory-set judgments for nonpaired stimuli) serves as a test for attitude-consistent guessing.
Most often, nonpaired stimuli were judged 'not remembered', and hence Attitude-set responses were given on the TBS task.
Yet, in those cases that participants falsely 'remembered' that a US had been paired with them (i.e., used the Memory button set), they reported a US valence that tended to be consistent with pre-experimental attitudes, `r apa_print(x1npM)$full_result$Valence1`.
However, as indicated by the above interaction with Response Set, this effect was smaller than for the Attitude-set judgments.
In sum, participants indeed showed an attitude-as-information bias on Memory-set responses: 
For a stimulus falsely judged as having been paired with a US, they relied on the evaluation of that stimulus to inform their memory judgments in 73% of cases.
Yet, Memory-set responses did not reflect attitudes as strongly as did Attitude-set responses, suggesting that participants may be able to influence the degree to which they allow pre-existing attitudes to inform their judgments.

Taken together, the results of the TBS task appropriately reflected the memory and attitude manipulations. 
Furthermore, we found no EC in the absence of memory.
In addition, an effect of attitude-as-information was found on the Memory-set responses.
